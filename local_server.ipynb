{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac1e4de27ca438c8276774cbdea6345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3863745c4f4269bcaf2de5230f3eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c103cbca3ca41e5b2d3f31ece8a3b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26dd69950c3430cb02b5c9c0adfcfe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 15:07:06.438223: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-07 15:07:06.438276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-07 15:07:06.502894: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-07 15:07:07.819462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550d79df9b964209990cc1a13aa74c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/587 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9b2da7a4ca4b3d844a764394714036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736ff4158c13487593420e97bd939346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1b7893fc9249a4a32b40bb068f7249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab7fb5d2f854e2bbad20a36c2f9c305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a29e87310cb4092930b804e42b1ed11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27c60b9ea4240e0b73217ba33753aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ba6fc26cde4fc9b75631bf211d6ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "import accelerate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "secret_value_0 = os.getenv(\"HF_TOKEN\")\n",
    "model_chat = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "tokenizer_chat = AutoTokenizer.from_pretrained(model_chat, token=secret_value_0)\n",
    "pipeline_chat = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_chat,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=secret_value_0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "secret_value_3 = os.getenv(\"TAVILY_API_KEY\")\n",
    "tavily_cli = TavilyClient(api_key=secret_value_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "secret_value_4 = os.getenv(\"UPSTASH_PD\")\n",
    "redis_client = redis.StrictRedis(host=\"usw1-organic-yak-33107.upstash.io\", port='33107', password=secret_value_4, decode_responses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ngrok = redis_client.get('ngrok_link')\n",
    "except Exception as e:\n",
    "    link = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ques(in_s, out_s, query):\n",
    "    prompt = \"\"\"\n",
    "    <s>[INST] <<SYS>>\n",
    "    You are a helpful assistant that helps users generate questions. You will receive one question, and the question will be in JSON format. It will contain the question stem itself, no more than 4 options to answer the question, the correct option for the particular question, and the subject or topic related to the question.\n",
    "    You will do your best to generate one question, which should be similar in topic, style and difficulty of the question provided to you. It should be in JSON Format, and provide the question stem itself, the options, and the correct option to select.\n",
    "    You should ONLY have the JSON dictionary in your response.\n",
    "    <</SYS>>\n",
    "\n",
    "    {} [/INST] {} </s><s>[INST] {} [/INST]\n",
    "    \"\"\".format(in_s, out_s, query)\n",
    "    result = pipeline_chat(prompt)\n",
    "    return result\n",
    "def get_ans(in_s, out_s, query):\n",
    "    prompt = \"\"\"\n",
    "    <s>[INST] <<SYS>>\n",
    "    You are a helpful assistant that helps label correct answers for question. You will receive one question, and the question will be in JSON format. It will contain the question stem itself, no more than 4 options to answer the question, and information to help you answer the question.\n",
    "    You will do your best to match the correct option. Your answer should be in JSON format, providing the correct answer. If the information provided states that the correct answer is something, not in the options, change one of the options to accomodate for the correct option.\n",
    "    You should ONLY have the JSON dictionary in your response. The answer you provide should only be equal to A, B, C, D nothing else!\n",
    "    <</SYS>>\n",
    "\n",
    "    {} [/INST] {} </s><s>[INST] {} [/INST]\n",
    "    \"\"\".format(in_s, out_s, query)\n",
    "    result = pipeline_chat(prompt)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_in=\"{'topic': 'Evolution', 'question': 'Man's evolution is defined as:', 'options': ['Dysversion', 'Anthropogenesis', 'Convergence', 'ontogenesis'], 'answer': 'B'}\"\n",
    "sample_out=\"{'question': 'Which of the following mechanisms is NOT a driving force of evolution?', 'options': ['Natural selection', 'Genetic drift', 'Lamarckism', 'Geographical barriers'], 'answer': 'C'}\"\n",
    "ques_in=\"{'topic': 'Kinematics', 'question': 'Which of the following values is a vector?', 'options': ['Speed', 'Distance', 'Position', 'Velocity'], 'info': 'D. Velocity is a vector, as it has a direction'}\"\n",
    "ques_out=\"{'topic': 'Kinematics', 'question': 'Which of the following values is a vector?', 'options': ['Speed', 'Distance', 'Position', 'Velocity'], 'info': 'D. Velocity is a vector, as it has a direction', 'answer': 'D'}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      "Access your Flask app at: NgrokTunnel: \"https://b3bd-174-160-2-215.ngrok-free.app\" -> \"http://localhost:6060\"\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:6060\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [07/Mar/2024 16:13:10] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [07/Mar/2024 16:13:10] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "import threading\n",
    "from pyngrok import ngrok\n",
    "import json\n",
    "from flask_cors import CORS\n",
    "import re\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "@app.route('/run', methods=['GET'])\n",
    "def hello_world():\n",
    "    try:\n",
    "        data_in = request.args.get('msg')\n",
    "        data_out = get_ques(sample_in, sample_out, data_in)[0][\"generated_text\"].split(\"[/INST]\\n     \")[1]\n",
    "        pattern = r\"(?<=\\W)'(.*?)'(?=\\W)\"\n",
    "        output_string = re.sub(pattern, lambda x: f'\"{x.group(1)}\"', data_out)\n",
    "        dict_ans = json.loads(output_string)\n",
    "        query_tavily = dict_ans[\"question\"] + \": \"\n",
    "        opt_str = \"ABCD\"\n",
    "        for n in range(4):\n",
    "            query_tavily += opt_str[n] + \". \" + dict_ans[\"options\"][n] + \"  \"\n",
    "        context = tavily_cli.qna_search(query=query_tavily[:-2])\n",
    "        try:\n",
    "            del dict_ans[\"answer\"]\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        dict_ans[\"info\"] = context\n",
    "        final_response = get_ans(ques_in, ques_out, dict_ans)[0][\"generated_text\"].split(\"[/INST]\\n     \")[1]\n",
    "        return jsonify({\"output\": final_response})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def run_flask():\n",
    "    app.run(port=6060)\n",
    "secret_value_1 = os.getenv(\"NGROK_API\")\n",
    "ngrok_auth_token = secret_value_1\n",
    "ngrok.set_auth_token(ngrok_auth_token)\n",
    "public_url = ngrok.connect(6060)\n",
    "threading.Thread(target=run_flask).start()\n",
    "pattern_link = r\"https?://\\S+\"\n",
    "matches = str(re.findall(pattern_link, str(public_url))[0])[:-1]\n",
    "redis_client.set('ngrok_link', matches+\"/\")\n",
    "print(f\"Access your Flask app at: {public_url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
